{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-01-14T12:22:44.604421Z","iopub.status.busy":"2023-01-14T12:22:44.604003Z","iopub.status.idle":"2023-01-14T12:22:52.657369Z","shell.execute_reply":"2023-01-14T12:22:52.656299Z","shell.execute_reply.started":"2023-01-14T12:22:44.604335Z"},"trusted":true},"outputs":[],"source":["import os\n","import json\n","import csv\n","import random\n","import pickle\n","import cv2\n","import numpy as np\n","import time\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from scipy.ndimage.measurements import label\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","from keras.preprocessing.image import ImageDataGenerator, load_img, array_to_img, img_to_array\n","from PIL import Image\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:22:52.665359Z","iopub.status.busy":"2023-01-14T12:22:52.662777Z","iopub.status.idle":"2023-01-14T12:22:52.670887Z","shell.execute_reply":"2023-01-14T12:22:52.669995Z","shell.execute_reply.started":"2023-01-14T12:22:52.665319Z"},"trusted":true},"outputs":[],"source":["# import torch_xla\n","# import torch_xla.core.xla_model as xm"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:22:52.678582Z","iopub.status.busy":"2023-01-14T12:22:52.676016Z","iopub.status.idle":"2023-01-14T12:22:52.698571Z","shell.execute_reply":"2023-01-14T12:22:52.697620Z","shell.execute_reply.started":"2023-01-14T12:22:52.678545Z"},"trusted":true},"outputs":[],"source":["class GlaucomaDataset(Dataset):\n","\n","    def __init__(self, root_dir, split='train', output_size=(256,256)):\n","        self.output_size = output_size\n","        self.root_dir = root_dir\n","        self.split = split\n","        self.images = []\n","        self.segs = []\n","        # Load data index\n","        for direct in self.root_dir:\n","            self.image_filenames = []\n","            for path in os.listdir(os.path.join(direct, \"Images_Square\")):\n","                if(not path.startswith('.')):\n","                    self.image_filenames.append(path)\n","\n","\n","            for k in range(len(self.image_filenames)):\n","                print('Loading {} image {}/{}...'.format(split, k, len(self.image_filenames)), end='\\r')\n","                img_name = os.path.join(direct, \"Images_Square\", self.image_filenames[k])\n","                #img = remove_nerves(np.array(Image.open(img_name).convert('RGB'))).astype(np.float32)\n","                img = np.array(Image.open(img_name).convert('RGB'))\n","                img = transforms.functional.to_tensor(img)\n","                img = transforms.functional.resize(img, output_size, interpolation=Image.Resampling.BILINEAR)\n","                self.images.append(img)\n","            if split != 'test':\n","                for k in range(len(self.image_filenames)):\n","                    print('Loading {} segmentation {}/{}...'.format(split, k, len(self.image_filenames)), end='\\r')\n","                    seg_name = os.path.join(direct, \"Masks_Square\", self.image_filenames[k][:-3] + \"png\")\n","                    mask = np.array(Image.open(seg_name, mode='r'))\n","                    od = (mask==1.).astype(np.float32)\n","                    oc = (mask==2.).astype(np.float32)\n","                    od = torch.from_numpy(od[None,:,:])\n","                    oc = torch.from_numpy(oc[None,:,:])\n","                    od = transforms.functional.resize(od, output_size, interpolation=Image.Resampling.NEAREST)\n","                    oc = transforms.functional.resize(oc, output_size, interpolation=Image.Resampling.NEAREST)\n","                    self.segs.append(torch.cat([od, oc], dim=0))\n","\n","            print('Succesfully loaded {} dataset.'.format(split) + ' '*50)\n","            \n","            \n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img = self.images[idx]\n","        if self.split == 'test':\n","            return img\n","        else:\n","            seg = self.segs[idx]\n","            return img, seg"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:22:52.707058Z","iopub.status.busy":"2023-01-14T12:22:52.704467Z","iopub.status.idle":"2023-01-14T12:22:52.717717Z","shell.execute_reply":"2023-01-14T12:22:52.716758Z","shell.execute_reply.started":"2023-01-14T12:22:52.707023Z"},"trusted":true},"outputs":[],"source":["def remove_nerves(image):\n","    img = array_to_img(image)\n","    \n","    img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2RGB)\n","    # convert image to grayScale\n","    grayScale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","   \n","    # kernel for morphologyEx\n","    kernel = cv2.getStructuringElement(1,(17,17))\n","   \n","    # apply MORPH_BLACKHAT to grayScale image\n","    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n","  \n","    # apply thresholding to blackhat\n","    _,threshold = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n","\n","    # inpaint with original image and threshold image\n","    final_image = cv2.inpaint(img,threshold,1,cv2.INPAINT_TELEA)\n","    final_image = cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB)\n","    \n","    return final_image.astype(np.float64)/255.0"]},{"cell_type":"markdown","metadata":{},"source":["# Metrics"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:22:52.725281Z","iopub.status.busy":"2023-01-14T12:22:52.722622Z","iopub.status.idle":"2023-01-14T12:22:52.740019Z","shell.execute_reply":"2023-01-14T12:22:52.738963Z","shell.execute_reply.started":"2023-01-14T12:22:52.725246Z"},"trusted":true},"outputs":[],"source":["EPS = 1e-7\n","\n","def compute_dice_coef(input, target):\n","    '''\n","    Compute dice score metric.\n","    '''\n","    batch_size = input.shape[0]\n","    return sum([dice_coef_sample(input[k,:,:], target[k,:,:]) for k in range(batch_size)])/batch_size\n","\n","def dice_coef_sample(input, target):\n","    iflat = input.contiguous().view(-1)\n","    tflat = target.contiguous().view(-1)\n","    intersection = (iflat * tflat).sum()\n","    return (2. * intersection) / (iflat.sum() + tflat.sum())\n","\n","\n","def vertical_diameter(binary_segmentation):\n","    '''\n","    Get the vertical diameter from a binary segmentation.\n","    The vertical diameter is defined as the \"fattest\" area of the binary_segmentation parameter.\n","    '''\n","\n","    # get the sum of the pixels in the vertical axis\n","    vertical_axis_diameter = np.sum(binary_segmentation, axis=1)\n","\n","    # pick the maximum value\n","    diameter = np.max(vertical_axis_diameter, axis=1)\n","\n","    # return it\n","    return diameter\n","\n","\n","\n","def vertical_cup_to_disc_ratio(od, oc):\n","    '''\n","    Compute the vertical cup-to-disc ratio from a given labelling map.\n","    '''\n","    # compute the cup diameter\n","    cup_diameter = vertical_diameter(oc)\n","    # compute the disc diameter\n","    disc_diameter = vertical_diameter(od)\n","\n","    return cup_diameter / (disc_diameter + EPS)\n","\n","def compute_vCDR_error(pred_od, pred_oc, gt_od, gt_oc):\n","    '''\n","    Compute vCDR prediction error, along with predicted vCDR and ground truth vCDR.\n","    '''\n","    pred_vCDR = vertical_cup_to_disc_ratio(pred_od, pred_oc)\n","    gt_vCDR = vertical_cup_to_disc_ratio(gt_od, gt_oc)\n","    vCDR_err = np.mean(np.abs(gt_vCDR - pred_vCDR))\n","    return vCDR_err, pred_vCDR, gt_vCDR\n","\n","\n","def classif_eval(classif_preds, classif_gts):\n","    '''\n","    Compute AUC classification score.\n","    '''\n","    auc = roc_auc_score(classif_gts, classif_preds)\n","    return auc\n"]},{"cell_type":"markdown","metadata":{},"source":["# Post Processing"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:22:52.747817Z","iopub.status.busy":"2023-01-14T12:22:52.745256Z","iopub.status.idle":"2023-01-14T12:22:52.757649Z","shell.execute_reply":"2023-01-14T12:22:52.756615Z","shell.execute_reply.started":"2023-01-14T12:22:52.747782Z"},"trusted":true},"outputs":[],"source":["def refine_seg(pred):\n","    '''\n","    Only retain the biggest connected component of a segmentation map.\n","    '''\n","    np_pred = pred.numpy()\n","        \n","    largest_ccs = []\n","    for i in range(np_pred.shape[0]):\n","        labeled, ncomponents = label(np_pred[i,:,:])\n","        bincounts = np.bincount(labeled.flat)[1:]\n","        if len(bincounts) == 0:\n","            largest_cc = labeled == 0\n","        else:\n","            largest_cc = labeled == np.argmax(bincounts)+1\n","        largest_cc = torch.tensor(largest_cc, dtype=torch.float32)\n","        largest_ccs.append(largest_cc)\n","    largest_ccs = torch.stack(largest_ccs)\n","    \n","    return largest_ccs"]},{"cell_type":"markdown","metadata":{},"source":["# Network"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:22:52.765413Z","iopub.status.busy":"2023-01-14T12:22:52.762759Z","iopub.status.idle":"2023-01-14T12:23:05.801868Z","shell.execute_reply":"2023-01-14T12:23:05.800661Z","shell.execute_reply.started":"2023-01-14T12:22:52.765376Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install -q ml_collections"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:23:05.805459Z","iopub.status.busy":"2023-01-14T12:23:05.805072Z","iopub.status.idle":"2023-01-14T12:23:05.856396Z","shell.execute_reply":"2023-01-14T12:23:05.855529Z","shell.execute_reply.started":"2023-01-14T12:23:05.805422Z"},"trusted":true},"outputs":[],"source":["import ml_collections\n","\n","def get_b16_config():\n","    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n","    config = ml_collections.ConfigDict()\n","    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n","    config.hidden_size = 768\n","    config.transformer = ml_collections.ConfigDict()\n","    config.transformer.mlp_dim = 3072\n","    config.transformer.num_heads = 12\n","    config.transformer.num_layers = 12\n","    config.transformer.attention_dropout_rate = 0.0\n","    config.transformer.dropout_rate = 0.1\n","\n","    config.classifier = 'seg'\n","    config.representation_size = None\n","    config.resnet_pretrained_path = None\n","    config.pretrained_path = '../input/project-transunet/project_TransUNet/model/vit_checkpoint/imagenet21k/ViT-B_16.npz'\n","    config.patch_size = 16\n","\n","    config.decoder_channels = (256, 128, 64, 16)\n","    config.n_classes = 2\n","    config.activation = 'softmax'\n","    return config\n","\n","\n","def get_testing():\n","    \"\"\"Returns a minimal configuration for testing.\"\"\"\n","    config = ml_collections.ConfigDict()\n","    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n","    config.hidden_size = 1\n","    config.transformer = ml_collections.ConfigDict()\n","    config.transformer.mlp_dim = 1\n","    config.transformer.num_heads = 1\n","    config.transformer.num_layers = 1\n","    config.transformer.attention_dropout_rate = 0.0\n","    config.transformer.dropout_rate = 0.1\n","    config.classifier = 'token'\n","    config.representation_size = None\n","    return config\n","\n","def get_r50_b16_config():\n","    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n","    config = get_b16_config()\n","    config.patches.grid = (16, 16)\n","    config.resnet = ml_collections.ConfigDict()\n","    config.resnet.num_layers = (3, 4, 9)\n","    config.resnet.width_factor = 1\n","\n","    config.classifier = 'seg'\n","    config.pretrained_path = '../input/project-transunet/project_TransUNet/model/vit_checkpoint/imagenet21k/R50+ViT-B_16.npz'\n","    config.decoder_channels = (256, 128, 64, 16)\n","    config.skip_channels = [512, 256, 64, 16]\n","    config.n_classes = 2\n","    config.n_skip = 3\n","    config.activation = 'softmax'\n","\n","    return config\n","\n","\n","def get_b32_config():\n","    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n","    config = get_b16_config()\n","    config.patches.size = (32, 32)\n","    config.pretrained_path = '../input/project-transunet/project_TransUNet/model/vit_checkpoint/imagenet21k/ViT-B_32.npz'\n","    return config\n","\n","\n","def get_l16_config():\n","    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n","    config = ml_collections.ConfigDict()\n","    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n","    config.hidden_size = 1024\n","    config.transformer = ml_collections.ConfigDict()\n","    config.transformer.mlp_dim = 4096\n","    config.transformer.num_heads = 16\n","    config.transformer.num_layers = 24\n","    config.transformer.attention_dropout_rate = 0.0\n","    config.transformer.dropout_rate = 0.1\n","    config.representation_size = None\n","\n","    # custom\n","    config.classifier = 'seg'\n","    config.resnet_pretrained_path = None\n","    config.pretrained_path = '../input/project-transunet/project_TransUNet/model/vit_checkpoint/imagenet21k/ViT-L_16.npz'\n","    config.decoder_channels = (256, 128, 64, 16)\n","    config.n_classes = 2\n","    config.activation = 'softmax'\n","    return config\n","\n","\n","def get_r50_l16_config():\n","    \"\"\"Returns the Resnet50 + ViT-L/16 configuration. customized \"\"\"\n","    config = get_l16_config()\n","    config.patches.grid = (16, 16)\n","    config.resnet = ml_collections.ConfigDict()\n","    config.resnet.num_layers = (3, 4, 9)\n","    config.resnet.width_factor = 1\n","\n","    config.classifier = 'seg'\n","    config.resnet_pretrained_path = '../input/project-transunet/project_TransUNet/model/vit_checkpoint/imagenet21k/R50+ViT-B_16.npz'\n","    config.decoder_channels = (256, 128, 64, 16)\n","    config.skip_channels = [512, 256, 64, 16]\n","    config.n_classes = 2\n","    config.activation = 'softmax'\n","    return config\n","\n","\n","def get_l32_config():\n","    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n","    config = get_l16_config()\n","    config.patches.size = (32, 32)\n","    return config\n","\n","\n","def get_h14_config():\n","    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n","    config = ml_collections.ConfigDict()\n","    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n","    config.hidden_size = 1280\n","    config.transformer = ml_collections.ConfigDict()\n","    config.transformer.mlp_dim = 5120\n","    config.transformer.num_heads = 16\n","    config.transformer.num_layers = 32\n","    config.transformer.attention_dropout_rate = 0.0\n","    config.transformer.dropout_rate = 0.1\n","    config.classifier = 'token'\n","    config.representation_size = None\n","\n","    return config"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:23:05.858393Z","iopub.status.busy":"2023-01-14T12:23:05.858058Z","iopub.status.idle":"2023-01-14T12:23:05.892948Z","shell.execute_reply":"2023-01-14T12:23:05.891917Z","shell.execute_reply.started":"2023-01-14T12:23:05.858351Z"},"trusted":true},"outputs":[],"source":["import math\n","\n","from os.path import join as pjoin\n","from collections import OrderedDict\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","def np2th(weights, conv=False):\n","    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n","    if conv:\n","        weights = weights.transpose([3, 2, 0, 1])\n","    return torch.from_numpy(weights)\n","\n","\n","class StdConv2d(nn.Conv2d):\n","\n","    def forward(self, x):\n","        w = self.weight\n","        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n","        w = (w - m) / torch.sqrt(v + 1e-5)\n","        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n","                        self.dilation, self.groups)\n","\n","\n","def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n","    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n","                     padding=1, bias=bias, groups=groups)\n","\n","\n","def conv1x1(cin, cout, stride=1, bias=False):\n","    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n","                     padding=0, bias=bias)\n","\n","\n","class PreActBottleneck(nn.Module):\n","    \"\"\"Pre-activation (v2) bottleneck block.\n","    \"\"\"\n","\n","    def __init__(self, cin, cout=None, cmid=None, stride=1):\n","        super().__init__()\n","        cout = cout or cin\n","        cmid = cmid or cout//4\n","\n","        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n","        self.conv1 = conv1x1(cin, cmid, bias=False)\n","        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n","        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n","        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n","        self.conv3 = conv1x1(cmid, cout, bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        if (stride != 1 or cin != cout):\n","            # Projection also with pre-activation according to paper.\n","            self.downsample = conv1x1(cin, cout, stride, bias=False)\n","            self.gn_proj = nn.GroupNorm(cout, cout)\n","\n","    def forward(self, x):\n","\n","        # Residual branch\n","        residual = x\n","        if hasattr(self, 'downsample'):\n","            residual = self.downsample(x)\n","            residual = self.gn_proj(residual)\n","\n","        # Unit's branch\n","        y = self.relu(self.gn1(self.conv1(x)))\n","        y = self.relu(self.gn2(self.conv2(y)))\n","        y = self.gn3(self.conv3(y))\n","\n","        y = self.relu(residual + y)\n","        return y\n","\n","    def load_from(self, weights, n_block, n_unit):\n","        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n","        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n","        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n","\n","        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n","        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n","\n","        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n","        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n","\n","        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n","        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n","\n","        self.conv1.weight.copy_(conv1_weight)\n","        self.conv2.weight.copy_(conv2_weight)\n","        self.conv3.weight.copy_(conv3_weight)\n","\n","        self.gn1.weight.copy_(gn1_weight.view(-1))\n","        self.gn1.bias.copy_(gn1_bias.view(-1))\n","\n","        self.gn2.weight.copy_(gn2_weight.view(-1))\n","        self.gn2.bias.copy_(gn2_bias.view(-1))\n","\n","        self.gn3.weight.copy_(gn3_weight.view(-1))\n","        self.gn3.bias.copy_(gn3_bias.view(-1))\n","\n","        if hasattr(self, 'downsample'):\n","            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n","            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n","            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n","\n","            self.downsample.weight.copy_(proj_conv_weight)\n","            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n","            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))\n","\n","class ResNetV2(nn.Module):\n","    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n","\n","    def __init__(self, block_units, width_factor):\n","        super().__init__()\n","        width = int(64 * width_factor)\n","        self.width = width\n","\n","        self.root = nn.Sequential(OrderedDict([\n","            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n","            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n","            ('relu', nn.ReLU(inplace=True)),\n","            # ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n","        ]))\n","\n","        self.body = nn.Sequential(OrderedDict([\n","            ('block1', nn.Sequential(OrderedDict(\n","                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n","                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n","                ))),\n","            ('block2', nn.Sequential(OrderedDict(\n","                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n","                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n","                ))),\n","            ('block3', nn.Sequential(OrderedDict(\n","                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n","                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n","                ))),\n","        ]))\n","\n","    def forward(self, x):\n","        features = []\n","        b, c, in_size, _ = x.size()\n","        x = self.root(x)\n","        features.append(x)\n","        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)(x)\n","        for i in range(len(self.body)-1):\n","            x = self.body[i](x)\n","            right_size = int(in_size / 4 / (i+1))\n","            if x.size()[2] != right_size:\n","                pad = right_size - x.size()[2]\n","                assert pad < 3 and pad > 0, \"x {} should {}\".format(x.size(), right_size)\n","                feat = torch.zeros((b, x.size()[1], right_size, right_size), device=x.device)\n","                feat[:, :, 0:x.size()[2], 0:x.size()[3]] = x[:]\n","            else:\n","                feat = x\n","            features.append(feat)\n","        x = self.body[-1](x)\n","        return x, features[::-1]"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:23:05.898938Z","iopub.status.busy":"2023-01-14T12:23:05.898581Z","iopub.status.idle":"2023-01-14T12:23:05.973639Z","shell.execute_reply":"2023-01-14T12:23:05.972741Z","shell.execute_reply.started":"2023-01-14T12:23:05.898908Z"},"trusted":true},"outputs":[],"source":["# coding=utf-8\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import copy\n","import logging\n","import math\n","\n","from os.path import join as pjoin\n","\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n","from torch.nn.modules.utils import _pair\n","from scipy import ndimage\n","\n","logger = logging.getLogger(__name__)\n","\n","ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n","ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n","ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n","ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n","FC_0 = \"MlpBlock_3/Dense_0\"\n","FC_1 = \"MlpBlock_3/Dense_1\"\n","ATTENTION_NORM = \"LayerNorm_0\"\n","MLP_NORM = \"LayerNorm_2\"\n","\n","\n","def np2th(weights, conv=False):\n","    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n","    if conv:\n","        weights = weights.transpose([3, 2, 0, 1])\n","    return torch.from_numpy(weights)\n","\n","\n","def swish(x):\n","    return x * torch.sigmoid(x)\n","\n","\n","ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, config, vis):\n","        super(Attention, self).__init__()\n","        self.vis = vis\n","        self.num_attention_heads = config.transformer[\"num_heads\"]\n","        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.query = Linear(config.hidden_size, self.all_head_size)\n","        self.key = Linear(config.hidden_size, self.all_head_size)\n","        self.value = Linear(config.hidden_size, self.all_head_size)\n","\n","        self.out = Linear(config.hidden_size, config.hidden_size)\n","        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n","        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n","\n","        self.softmax = Softmax(dim=-1)\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, hidden_states):\n","        mixed_query_layer = self.query(hidden_states)\n","        mixed_key_layer = self.key(hidden_states)\n","        mixed_value_layer = self.value(hidden_states)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer)\n","        key_layer = self.transpose_for_scores(mixed_key_layer)\n","        value_layer = self.transpose_for_scores(mixed_value_layer)\n","\n","        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","        attention_probs = self.softmax(attention_scores)\n","        weights = attention_probs if self.vis else None\n","        attention_probs = self.attn_dropout(attention_probs)\n","\n","        context_layer = torch.matmul(attention_probs, value_layer)\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","        attention_output = self.out(context_layer)\n","        attention_output = self.proj_dropout(attention_output)\n","        return attention_output, weights\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, config):\n","        super(Mlp, self).__init__()\n","        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n","        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n","        self.act_fn = ACT2FN[\"gelu\"]\n","        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n","\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        nn.init.xavier_uniform_(self.fc1.weight)\n","        nn.init.xavier_uniform_(self.fc2.weight)\n","        nn.init.normal_(self.fc1.bias, std=1e-6)\n","        nn.init.normal_(self.fc2.bias, std=1e-6)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act_fn(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        x = self.dropout(x)\n","        return x\n","\n","\n","class Embeddings(nn.Module):\n","    \"\"\"Construct the embeddings from patch, position embeddings.\n","    \"\"\"\n","\n","    def __init__(self, config, img_size, in_channels=3):\n","        super(Embeddings, self).__init__()\n","        self.hybrid = None\n","        self.config = config\n","        img_size = _pair(img_size)\n","\n","        if config.patches.get(\"grid\") is not None:  # ResNet\n","            grid_size = config.patches[\"grid\"]\n","            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n","            patch_size_real = (patch_size[0] * 16, patch_size[1] * 16)\n","            n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1])\n","            self.hybrid = True\n","        else:\n","            patch_size = _pair(config.patches[\"size\"])\n","            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n","            self.hybrid = False\n","\n","        if self.hybrid:\n","            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers, width_factor=config.resnet.width_factor)\n","            in_channels = self.hybrid_model.width * 16\n","        self.patch_embeddings = Conv2d(in_channels=in_channels,\n","                                       out_channels=config.hidden_size,\n","                                       kernel_size=patch_size,\n","                                       stride=patch_size)\n","        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, config.hidden_size))\n","\n","        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n","\n","    def forward(self, x):\n","        if self.hybrid:\n","            x, features = self.hybrid_model(x)\n","        else:\n","            features = None\n","        x = self.patch_embeddings(x)  # (B, hidden. n_patches^(1/2), n_patches^(1/2))\n","        x = x.flatten(2)\n","        x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n","\n","        embeddings = x + self.position_embeddings\n","        embeddings = self.dropout(embeddings)\n","        return embeddings, features\n","\n","\n","class Block(nn.Module):\n","    def __init__(self, config, vis):\n","        super(Block, self).__init__()\n","        self.hidden_size = config.hidden_size\n","        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n","        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n","        self.ffn = Mlp(config)\n","        self.attn = Attention(config, vis)\n","\n","    def forward(self, x):\n","        h = x\n","        x = self.attention_norm(x)\n","        x, weights = self.attn(x)\n","        x = x + h\n","\n","        h = x\n","        x = self.ffn_norm(x)\n","        x = self.ffn(x)\n","        x = x + h\n","        return x, weights\n","\n","    def load_from(self, weights, n_block):\n","        ROOT = f\"Transformer/encoderblock_{n_block}\"\n","        with torch.no_grad():\n","            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n","            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n","            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n","            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n","\n","            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n","            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n","            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n","            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n","\n","            self.attn.query.weight.copy_(query_weight)\n","            self.attn.key.weight.copy_(key_weight)\n","            self.attn.value.weight.copy_(value_weight)\n","            self.attn.out.weight.copy_(out_weight)\n","            self.attn.query.bias.copy_(query_bias)\n","            self.attn.key.bias.copy_(key_bias)\n","            self.attn.value.bias.copy_(value_bias)\n","            self.attn.out.bias.copy_(out_bias)\n","\n","            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n","            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n","            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n","            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n","\n","            self.ffn.fc1.weight.copy_(mlp_weight_0)\n","            self.ffn.fc2.weight.copy_(mlp_weight_1)\n","            self.ffn.fc1.bias.copy_(mlp_bias_0)\n","            self.ffn.fc2.bias.copy_(mlp_bias_1)\n","\n","            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n","            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n","            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n","            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, config, vis):\n","        super(Encoder, self).__init__()\n","        self.vis = vis\n","        self.layer = nn.ModuleList()\n","        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n","        for _ in range(config.transformer[\"num_layers\"]):\n","            layer = Block(config, vis)\n","            self.layer.append(copy.deepcopy(layer))\n","\n","    def forward(self, hidden_states):\n","        attn_weights = []\n","        for layer_block in self.layer:\n","            hidden_states, weights = layer_block(hidden_states)\n","            if self.vis:\n","                attn_weights.append(weights)\n","        encoded = self.encoder_norm(hidden_states)\n","        return encoded, attn_weights\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, config, img_size, vis):\n","        super(Transformer, self).__init__()\n","        self.embeddings = Embeddings(config, img_size=img_size)\n","        self.encoder = Encoder(config, vis)\n","\n","    def forward(self, input_ids):\n","        embedding_output, features = self.embeddings(input_ids)\n","        encoded, attn_weights = self.encoder(embedding_output)  # (B, n_patch, hidden)\n","        return encoded, attn_weights, features\n","\n","\n","class Conv2dReLU(nn.Sequential):\n","    def __init__(\n","            self,\n","            in_channels,\n","            out_channels,\n","            kernel_size,\n","            padding=0,\n","            stride=1,\n","            use_batchnorm=True,\n","    ):\n","        conv = nn.Conv2d(\n","            in_channels,\n","            out_channels,\n","            kernel_size,\n","            stride=(stride, stride),\n","            padding=padding,\n","            bias=not (use_batchnorm),\n","        )\n","        relu = nn.ReLU(inplace=True)\n","\n","        bn = nn.BatchNorm2d(out_channels)\n","\n","        super(Conv2dReLU, self).__init__(conv, bn, relu)\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(\n","            self,\n","            in_channels,\n","            out_channels,\n","            skip_channels=0,\n","            use_batchnorm=True,\n","    ):\n","        super().__init__()\n","        self.conv1 = Conv2dReLU(\n","            in_channels + skip_channels,\n","            out_channels,\n","            kernel_size=3,\n","            padding=1,\n","            use_batchnorm=use_batchnorm,\n","        )\n","        self.conv2 = Conv2dReLU(\n","            out_channels,\n","            out_channels,\n","            kernel_size=3,\n","            padding=1,\n","            use_batchnorm=use_batchnorm,\n","        )\n","        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n","\n","    def forward(self, x, skip=None):\n","        x = self.up(x)\n","        if skip is not None:\n","            x = torch.cat([x, skip], dim=1)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        return x\n","\n","\n","class SegmentationHead(nn.Sequential):\n","\n","    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n","        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size), padding=kernel_size // 2)\n","        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n","        super().__init__(conv2d, upsampling)\n","\n","\n","class DecoderCup(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        head_channels = 512\n","        self.conv_more = Conv2dReLU(\n","            config.hidden_size,\n","            head_channels,\n","            kernel_size=3,\n","            padding=1,\n","            use_batchnorm=True,\n","        )\n","        decoder_channels = config.decoder_channels\n","        in_channels = [head_channels] + list(decoder_channels[:-1])\n","        out_channels = decoder_channels\n","\n","        if self.config.n_skip != 0:\n","            skip_channels = self.config.skip_channels\n","            for i in range(4 - self.config.n_skip):  # re-select the skip channels according to n_skip\n","                skip_channels[3 - i] = 0\n","\n","        else:\n","            skip_channels = [0, 0, 0, 0]\n","\n","        blocks = [\n","            DecoderBlock(in_ch, out_ch, sk_ch) for in_ch, out_ch, sk_ch in zip(in_channels, out_channels, skip_channels)\n","        ]\n","        self.blocks = nn.ModuleList(blocks)\n","\n","    def forward(self, hidden_states, features=None):\n","        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n","        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n","        x = hidden_states.permute(0, 2, 1)\n","        x = x.contiguous().view(B, hidden, h, w)\n","        x = self.conv_more(x)\n","        for i, decoder_block in enumerate(self.blocks):\n","            if features is not None:\n","                skip = features[i] if (i < self.config.n_skip) else None\n","            else:\n","                skip = None\n","            x = decoder_block(x, skip=skip)\n","        return x\n","\n","\n","class VisionTransformer(nn.Module):\n","    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n","        super(VisionTransformer, self).__init__()\n","        self.num_classes = num_classes\n","        self.zero_head = zero_head\n","        self.classifier = config.classifier\n","        self.transformer = Transformer(config, img_size, vis)\n","        self.decoder = DecoderCup(config)\n","        self.segmentation_head = SegmentationHead(\n","            in_channels=config['decoder_channels'][-1],\n","            out_channels=config['n_classes'],\n","            kernel_size=3,\n","        )\n","        self.config = config\n","\n","    def forward(self, x):\n","        if x.size()[1] == 1:\n","            x = x.repeat(1, 3, 1, 1)\n","        x, attn_weights, features = self.transformer(x)  # (B, n_patch, hidden)\n","        x = self.decoder(x, features)\n","        logits = self.segmentation_head(x)\n","        return logits\n","\n","    def load_from(self, weights):\n","        with torch.no_grad():\n","\n","            res_weight = weights\n","            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n","            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n","\n","            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n","            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n","\n","            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n","\n","            posemb_new = self.transformer.embeddings.position_embeddings\n","            if posemb.size() == posemb_new.size():\n","                self.transformer.embeddings.position_embeddings.copy_(posemb)\n","            elif posemb.size()[1] - 1 == posemb_new.size()[1]:\n","                posemb = posemb[:, 1:]\n","                self.transformer.embeddings.position_embeddings.copy_(posemb)\n","            else:\n","                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n","                ntok_new = posemb_new.size(1)\n","                if self.classifier == \"seg\":\n","                    _, posemb_grid = posemb[:, :1], posemb[0, 1:]\n","                gs_old = int(np.sqrt(len(posemb_grid)))\n","                gs_new = int(np.sqrt(ntok_new))\n","                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n","                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n","                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n","                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)  # th2np\n","                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n","                posemb = posemb_grid\n","                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n","\n","            # Encoder whole\n","            for bname, block in self.transformer.encoder.named_children():\n","                for uname, unit in block.named_children():\n","                    unit.load_from(weights, n_block=uname)\n","\n","            if self.transformer.embeddings.hybrid:\n","                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(\n","                    np2th(res_weight[\"conv_root/kernel\"], conv=True))\n","                gn_weight = np2th(res_weight[\"gn_root/scale\"]).view(-1)\n","                gn_bias = np2th(res_weight[\"gn_root/bias\"]).view(-1)\n","                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n","                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n","\n","                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n","                    for uname, unit in block.named_children():\n","                        unit.load_from(res_weight, n_block=bname, n_unit=uname)\n","\n","\n","CONFIGS = {\n","    'ViT-B_16': get_b16_config(),\n","    'ViT-B_32': get_b32_config(),\n","    'ViT-L_16': get_l16_config(),\n","    'ViT-L_32': get_l32_config(),\n","    'ViT-H_14': get_h14_config(),\n","    'R50-ViT-B_16': get_r50_b16_config(),\n","    'R50-ViT-L_16': get_r50_l16_config(),\n","    'testing': get_testing(),\n","}"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:23:05.975280Z","iopub.status.busy":"2023-01-14T12:23:05.974945Z","iopub.status.idle":"2023-01-14T12:23:05.992321Z","shell.execute_reply":"2023-01-14T12:23:05.991201Z","shell.execute_reply.started":"2023-01-14T12:23:05.975246Z"},"trusted":true},"outputs":[],"source":["import argparse\n","import logging\n","import os\n","import random\n","import numpy as np\n","import torch\n","import torch.backends.cudnn as cudnn\n","\n","zzroot_path = '../data/Synapse/train_npz'\n","zzdataset = 'Synapse'\n","zzlist_dir = './lists/lists_Synapse'\n","zznum_classes = 2\n","zzmax_iterations = 30000\n","zzmax_epochs = 40\n","zzbatch_size = 8\n","zzn_gpu = 1\n","zzdeterministic = 1\n","zzbase_lr = 0.01\n","zzimg_size = 256\n","zzseed = 1234\n","zzn_skip = 3\n","zzvit_name = 'R50-ViT-B_16'\n","zzvit_patches_size = 16\n","\n","\n","if not zzdeterministic:\n","    cudnn.benchmark = True\n","    cudnn.deterministic = False\n","else:\n","    cudnn.benchmark = False\n","    cudnn.deterministic = True\n","\n","random.seed(zzseed)\n","np.random.seed(zzseed)\n","torch.manual_seed(zzseed)\n","torch.cuda.manual_seed(zzseed)\n","dataset_name = zzdataset\n","dataset_config = {\n","    'Synapse': {\n","        'root_path': '../input/project-transunet/project_TransUNet/data/Synapse/train_npz',\n","        'list_dir': '../input/project-transunet/project_TransUNet/TransUNet/lists/lists_Synapse',\n","        'num_classes': 2,\n","    },\n","}\n","zznum_classes = 2\n","zzroot_path = dataset_config[dataset_name]['root_path']\n","zzlist_dir = dataset_config[dataset_name]['list_dir']\n","zzis_pretrain = False\n","zzexp = 'TU_' + dataset_name + str(zzimg_size)\n","snapshot_path = \"../model/{}/{}\".format(zzexp, 'TU')\n","snapshot_path = snapshot_path + '_pretrain' if zzis_pretrain else snapshot_path\n","snapshot_path += '_' + zzvit_name\n","snapshot_path = snapshot_path + '_skip' + str(zzn_skip)\n","snapshot_path = snapshot_path + '_vitpatch' + str(\n","    zzvit_patches_size) if zzvit_patches_size != 16 else snapshot_path\n","snapshot_path = snapshot_path + '_' + str(zzmax_iterations)[\n","                                      0:2] + 'k' if zzmax_iterations != 30000 else snapshot_path\n","snapshot_path = snapshot_path + '_epo' + str(zzmax_epochs) if zzmax_epochs != 30 else snapshot_path\n","snapshot_path = snapshot_path + '_bs' + str(zzbatch_size)\n","snapshot_path = snapshot_path + '_lr' + str(zzbase_lr) if zzbase_lr != 0.01 else snapshot_path\n","snapshot_path = snapshot_path + '_' + str(zzimg_size)\n","snapshot_path = snapshot_path + '_s' + str(zzseed) if zzseed != 1234 else snapshot_path\n","\n","if not os.path.exists(snapshot_path):\n","    os.makedirs(snapshot_path)\n","config_vit = CONFIGS[zzvit_name]\n","config_vit.n_classes = zznum_classes\n","config_vit.n_skip = zzn_skip\n","if zzvit_name.find('R50') != -1:\n","    config_vit.patches.grid = (int(zzimg_size / zzvit_patches_size), int(zzimg_size / zzvit_patches_size))\n","# model = VisionTransformer(config_vit, img_size=zzimg_size, num_classes=config_vit.n_classes).cuda()\n","# model.load_from(weights=np.load(config_vit.pretrained_path))"]},{"cell_type":"markdown","metadata":{},"source":["## UNet"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:23:05.994390Z","iopub.status.busy":"2023-01-14T12:23:05.994045Z","iopub.status.idle":"2023-01-14T12:23:06.015185Z","shell.execute_reply":"2023-01-14T12:23:06.014356Z","shell.execute_reply.started":"2023-01-14T12:23:05.994357Z"},"trusted":true},"outputs":[],"source":["class UNet(nn.Module):\n","    def __init__(self, n_channels=3, n_classes=2):\n","        super(UNet, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.epoch = 0\n","\n","        self.inc = DoubleConv(n_channels, 64)\n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        self.down4 = Down(512, 1024)\n","        self.down5 = Down(1024, 2048)\n","        factor = 2 \n","        self.down6 = Down(2048, 4096 // factor)\n","        self.up1 = Up(4096, 2048 // factor)\n","        self.up2 = Up(2048, 1024 // factor)\n","        self.up3 = Up(1024, 512 // factor)\n","        self.up4 = Up(512, 256 // factor)\n","        self.up5 = Up(256, 128 // factor)\n","        self.up6 = Up(128, 64)\n","        self.output_layer = OutConv(64, n_classes)\n","\n","\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x6 = self.down5(x5)\n","        x7 = self.down6(x6)\n","        out = self.up1(x7, x6)\n","        out = self.up2(out, x5)\n","        out = self.up3(out, x4)\n","        out = self.up4(out, x3)\n","        out = self.up5(out, x2)\n","        out = self.up6(out, x1)\n","        out = self.output_layer(out)\n","        out = torch.sigmoid(out)\n","        return out\n","\n","    \n","class DoubleConv(nn.Module):\n","    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","            \n","        )\n","        \n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    \"\"\"Downscaling with maxpool then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","            \n","        )\n","        \n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","\n","class Up(nn.Module):\n","    \"\"\"Upscaling then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","\n","        # Use the normal convolutions to reduce the number of channels\n","        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n","        \n","\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        # input is CHW\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2])\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","        \n","\n","\n","class OutConv(nn.Module):\n","    '''\n","    Simple convolution.\n","    '''\n","    def __init__(self, in_channels, out_channels):\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        \n","\n","    def forward(self, x):\n","        return self.conv(x)"]},{"cell_type":"markdown","metadata":{},"source":["## Attention UNet"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:23:06.017077Z","iopub.status.busy":"2023-01-14T12:23:06.016709Z","iopub.status.idle":"2023-01-14T12:23:06.033905Z","shell.execute_reply":"2023-01-14T12:23:06.032513Z","shell.execute_reply.started":"2023-01-14T12:23:06.017041Z"},"trusted":true},"outputs":[],"source":["class AttU_Net(nn.Module):\n","    def __init__(self, n_channels=3, n_classes=2):\n","        super(AttU_Net,self).__init__()\n","        \n","        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n","\n","        self.Conv1 = conv_block(ch_in=n_channels,ch_out=64)\n","        self.Conv2 = conv_block(ch_in=64,ch_out=128)\n","        self.Conv3 = conv_block(ch_in=128,ch_out=256)\n","        self.Conv4 = conv_block(ch_in=256,ch_out=512)\n","        self.Conv5 = conv_block(ch_in=512,ch_out=1024)\n","\n","        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n","        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)\n","        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n","\n","        self.Up4 = up_conv(ch_in=512,ch_out=256)\n","        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)\n","        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n","        \n","        self.Up3 = up_conv(ch_in=256,ch_out=128)\n","        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)\n","        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n","        \n","        self.Up2 = up_conv(ch_in=128,ch_out=64)\n","        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)\n","        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n","\n","        self.Conv_1x1 = nn.Conv2d(64,n_classes,kernel_size=1,stride=1,padding=0)\n","\n","\n","    def forward(self,x):\n","        # encoding path\n","        x1 = self.Conv1(x)\n","\n","        x2 = self.Maxpool(x1)\n","        x2 = self.Conv2(x2)\n","        \n","        x3 = self.Maxpool(x2)\n","        x3 = self.Conv3(x3)\n","\n","        x4 = self.Maxpool(x3)\n","        x4 = self.Conv4(x4)\n","\n","        x5 = self.Maxpool(x4)\n","        x5 = self.Conv5(x5)\n","\n","        # decoding + concat path\n","        d5 = self.Up5(x5)\n","        x4 = self.Att5(g=d5,x=x4)\n","        d5 = torch.cat((x4,d5),dim=1)        \n","        d5 = self.Up_conv5(d5)\n","        \n","        d4 = self.Up4(d5)\n","        x3 = self.Att4(g=d4,x=x3)\n","        d4 = torch.cat((x3,d4),dim=1)\n","        d4 = self.Up_conv4(d4)\n","\n","        d3 = self.Up3(d4)\n","        x2 = self.Att3(g=d3,x=x2)\n","        d3 = torch.cat((x2,d3),dim=1)\n","        d3 = self.Up_conv3(d3)\n","\n","        d2 = self.Up2(d3)\n","        x1 = self.Att2(g=d2,x=x1)\n","        d2 = torch.cat((x1,d2),dim=1)\n","        d2 = self.Up_conv2(d2)\n","\n","        d1 = self.Conv_1x1(d2)\n","\n","        return d1"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:23:06.037361Z","iopub.status.busy":"2023-01-14T12:23:06.036738Z","iopub.status.idle":"2023-01-14T12:23:06.056879Z","shell.execute_reply":"2023-01-14T12:23:06.055996Z","shell.execute_reply.started":"2023-01-14T12:23:06.037328Z"},"trusted":true},"outputs":[],"source":["class conv_block(nn.Module):\n","    def __init__(self,ch_in,ch_out):\n","        super(conv_block,self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n","            nn.BatchNorm2d(ch_out),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n","            nn.BatchNorm2d(ch_out),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","\n","    def forward(self,x):\n","        x = self.conv(x)\n","        return x\n","\n","class up_conv(nn.Module):\n","    def __init__(self,ch_in,ch_out):\n","        super(up_conv,self).__init__()\n","        self.up = nn.Sequential(\n","            nn.Upsample(scale_factor=2),\n","            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n","\t\t    nn.BatchNorm2d(ch_out),\n","\t\t\tnn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self,x):\n","        x = self.up(x)\n","        return x\n","\n","class Recurrent_block(nn.Module):\n","    def __init__(self,ch_out,t=2):\n","        super(Recurrent_block,self).__init__()\n","        self.t = t\n","        self.ch_out = ch_out\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(ch_out,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n","\t\t    nn.BatchNorm2d(ch_out),\n","\t\t\tnn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self,x):\n","        for i in range(self.t):\n","\n","            if i==0:\n","                x1 = self.conv(x)\n","            \n","            x1 = self.conv(x+x1)\n","        return x1\n","        \n","class RRCNN_block(nn.Module):\n","    def __init__(self,ch_in,ch_out,t=2):\n","        super(RRCNN_block,self).__init__()\n","        self.RCNN = nn.Sequential(\n","            Recurrent_block(ch_out,t=t),\n","            Recurrent_block(ch_out,t=t)\n","        )\n","        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)\n","\n","    def forward(self,x):\n","        x = self.Conv_1x1(x)\n","        x1 = self.RCNN(x)\n","        return x+x1\n","\n","\n","class single_conv(nn.Module):\n","    def __init__(self,ch_in,ch_out):\n","        super(single_conv,self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n","            nn.BatchNorm2d(ch_out),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self,x):\n","        x = self.conv(x)\n","        return x\n","\n","class Attention_block(nn.Module):\n","    def __init__(self,F_g,F_l,F_int):\n","        super(Attention_block,self).__init__()\n","        self.W_g = nn.Sequential(\n","            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n","            nn.BatchNorm2d(F_int)\n","            )\n","        \n","        self.W_x = nn.Sequential(\n","            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n","            nn.BatchNorm2d(F_int)\n","        )\n","\n","        self.psi = nn.Sequential(\n","            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n","            nn.BatchNorm2d(1),\n","            nn.Sigmoid()\n","        )\n","        \n","        self.relu = nn.ReLU(inplace=True)\n","        \n","    def forward(self,g,x):\n","        g1 = self.W_g(g)\n","        x1 = self.W_x(x)\n","        psi = self.relu(g1+x1)\n","        psi = self.psi(psi)\n","\n","        return x*psi"]},{"cell_type":"markdown","metadata":{},"source":["# Settings"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:23:06.058673Z","iopub.status.busy":"2023-01-14T12:23:06.058318Z","iopub.status.idle":"2023-01-14T12:23:06.069527Z","shell.execute_reply":"2023-01-14T12:23:06.068777Z","shell.execute_reply.started":"2023-01-14T12:23:06.058639Z"},"trusted":true},"outputs":[],"source":["root_dirs = [ \"../input/glaucoma-datasets/ORIGA\",\"../input/glaucoma-datasets/G1020\"]\n","# root_dirs = [ \"../input/glaucoma-datasets/ORIGA\"]\n","val_dir = [ \"../input/glaucoma-datasets/REFUGE\"] \n","# val_dir = [ \"../input/glaucoma-datasets/G1020\"]\n","lr = 0.01\n","batch_size = 8\n","num_workers = 8\n","total_epoch = 20"]},{"cell_type":"markdown","metadata":{},"source":["# Load Data"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:23:06.072795Z","iopub.status.busy":"2023-01-14T12:23:06.072533Z","iopub.status.idle":"2023-01-14T12:27:13.337966Z","shell.execute_reply":"2023-01-14T12:27:13.336978Z","shell.execute_reply.started":"2023-01-14T12:23:06.072772Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading train image 13/650...\r"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py:424: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"]},{"name":"stdout","output_type":"stream","text":["Succesfully loaded train dataset.                                                  \n","Succesfully loaded train dataset.                                                  \n","Succesfully loaded val dataset.                                                  \n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}],"source":["train_set = GlaucomaDataset(root_dirs, \n","                          split='train')\n","\n","val_set = GlaucomaDataset(val_dir, \n","                        split='val')\n","\n","train_loader = DataLoader(train_set, \n","                          batch_size=batch_size, \n","                          shuffle=True, \n","                          num_workers=num_workers,\n","                          pin_memory=True,\n","                         )\n","\n","val_loader = DataLoader(val_set, \n","                        batch_size=batch_size, \n","                        shuffle=False, \n","                        num_workers=num_workers,\n","                        pin_memory=True,\n","                        )\n"]},{"cell_type":"markdown","metadata":{},"source":["# Init Model"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:27:13.340255Z","iopub.status.busy":"2023-01-14T12:27:13.339593Z","iopub.status.idle":"2023-01-14T12:27:27.277544Z","shell.execute_reply":"2023-01-14T12:27:27.276412Z","shell.execute_reply.started":"2023-01-14T12:27:13.340219Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# summary.summary(model, (3,512, 512))\n","!pip install -q segmentation_models_pytorch"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:27:27.281645Z","iopub.status.busy":"2023-01-14T12:27:27.281318Z","iopub.status.idle":"2023-01-14T12:27:29.632300Z","shell.execute_reply":"2023-01-14T12:27:29.631168Z","shell.execute_reply.started":"2023-01-14T12:27:27.281616Z"},"trusted":true},"outputs":[],"source":["import segmentation_models_pytorch as smp\n","import segmentation_models_pytorch.utils\n","\n","# Device GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Device TPU\n","# device = xm.xla_device()\n","# torch.set_default_tensor_type('torch.FloatTensor')\n","\n","# Network\n","# model = UNet(n_channels=3, n_classes=2).to(device)\n","# model = AttU_Net(n_channels=3, n_classes=2).to(device)\n","\n","\n","# Loss\n","# seg_loss = torch.nn.BCELoss(reduction='mean')\n","seg_loss = smp.losses.DiceLoss(mode='binary')\n","# seg_loss = DiceBCELoss()\n","\n","# Optimizer\n","# optimizer = optim.Adam(model.parameters(), lr=0.01)\n","# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:27:29.636557Z","iopub.status.busy":"2023-01-14T12:27:29.636250Z","iopub.status.idle":"2023-01-14T12:27:29.645885Z","shell.execute_reply":"2023-01-14T12:27:29.644489Z","shell.execute_reply.started":"2023-01-14T12:27:29.636531Z"},"trusted":true},"outputs":[],"source":["def save_report(model, bod, boc, bauc, train_time, valid_time, total_params):\n","    dict_n = {'model': model, 'best_od_dice': [bod], 'best_oc_dice': [boc], 'avg_dice': [(bod+boc)/2], 'best_auc': [bauc], 'train_time': [train_time], 'valid_time': [valid_time], 'total_params': total_params} \n","    df = pd.DataFrame(dict_n)\n","    \n","    if(not os.path.isfile('best_score.csv')):\n","        df.to_csv('best_score.csv',index=False)\n","    else:\n","        df2 = pd.read_csv('best_score.csv')\n","        df2.append(df).to_csv('best_score.csv',index=False)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:27:29.648094Z","iopub.status.busy":"2023-01-14T12:27:29.647381Z","iopub.status.idle":"2023-01-14T12:27:29.658421Z","shell.execute_reply":"2023-01-14T12:27:29.657525Z","shell.execute_reply.started":"2023-01-14T12:27:29.648059Z"},"trusted":true},"outputs":[],"source":["def get_n_params(model):\n","    pp=0\n","    for p in list(model.parameters()):\n","        nn=1\n","        for s in list(p.size()):\n","            nn = nn*s\n","        pp += nn\n","    return pp"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T12:27:29.662459Z","iopub.status.busy":"2023-01-14T12:27:29.661819Z","iopub.status.idle":"2023-01-14T13:24:11.625408Z","shell.execute_reply":"2023-01-14T13:24:11.623893Z","shell.execute_reply.started":"2023-01-14T12:27:29.662336Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"name":"stdout","output_type":"stream","text":["VALIDATION epoch 1                                                       \n","LOSSES: 0.9781 (train), 0.9770 (val)\n","OD segmentation (Dice Score): 0.2976 (train), 0.5141 (val)\n","OC segmentation (Dice Score): 0.2486 (train), 0.3908 (val)\n","vCDR error: 0.4990 (train), 0.2935 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 2                                                       \n","LOSSES: 0.9776 (train), 0.9777 (val)\n","OD segmentation (Dice Score): 0.3781 (train), 0.3215 (val)\n","OC segmentation (Dice Score): 0.2984 (train), 0.2247 (val)\n","vCDR error: 0.6872 (train), 0.3058 (val)\n","__________________________________________________\n","VALIDATION epoch 3                                                       \n","LOSSES: 0.9774 (train), 0.9767 (val)\n","OD segmentation (Dice Score): 0.4109 (train), 0.6721 (val)\n","OC segmentation (Dice Score): 0.3196 (train), 0.4465 (val)\n","vCDR error: 0.3307 (train), 0.2613 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 4                                                       \n","LOSSES: 0.9773 (train), 0.9768 (val)\n","OD segmentation (Dice Score): 0.4639 (train), 0.2720 (val)\n","OC segmentation (Dice Score): 0.3532 (train), 0.2212 (val)\n","vCDR error: 0.3426 (train), 0.3583 (val)\n","__________________________________________________\n","VALIDATION epoch 5                                                       \n","LOSSES: 0.9772 (train), 0.9767 (val)\n","OD segmentation (Dice Score): 0.4820 (train), 0.5917 (val)\n","OC segmentation (Dice Score): 0.3580 (train), 0.4174 (val)\n","vCDR error: 0.3084 (train), 0.2238 (val)\n","__________________________________________________\n","VALIDATION epoch 6                                                       \n","LOSSES: 0.9772 (train), 0.9767 (val)\n","OD segmentation (Dice Score): 0.4871 (train), 0.4182 (val)\n","OC segmentation (Dice Score): 0.3804 (train), 0.3294 (val)\n","vCDR error: 0.3098 (train), 0.2058 (val)\n","__________________________________________________\n","VALIDATION epoch 7                                                       \n","LOSSES: 0.9772 (train), 0.9766 (val)\n","OD segmentation (Dice Score): 0.4694 (train), 0.5909 (val)\n","OC segmentation (Dice Score): 0.3637 (train), 0.4371 (val)\n","vCDR error: 0.2932 (train), 0.2062 (val)\n","__________________________________________________\n","VALIDATION epoch 8                                                       \n","LOSSES: 0.9771 (train), 0.9765 (val)\n","OD segmentation (Dice Score): 0.5262 (train), 0.6201 (val)\n","OC segmentation (Dice Score): 0.3991 (train), 0.4527 (val)\n","vCDR error: 0.2838 (train), 0.2083 (val)\n","__________________________________________________\n","VALIDATION epoch 9                                                       \n","LOSSES: 0.9771 (train), 0.9765 (val)\n","OD segmentation (Dice Score): 0.5396 (train), 0.6515 (val)\n","OC segmentation (Dice Score): 0.4055 (train), 0.4780 (val)\n","vCDR error: 0.2806 (train), 0.2043 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 10                                                      \n","LOSSES: 0.9771 (train), 0.9766 (val)\n","OD segmentation (Dice Score): 0.5721 (train), 0.5445 (val)\n","OC segmentation (Dice Score): 0.4188 (train), 0.3470 (val)\n","vCDR error: 0.2719 (train), 0.2637 (val)\n","__________________________________________________\n","VALIDATION epoch 11                                                      \n","LOSSES: 0.9771 (train), 0.9766 (val)\n","OD segmentation (Dice Score): 0.5566 (train), 0.6714 (val)\n","OC segmentation (Dice Score): 0.4119 (train), 0.4915 (val)\n","vCDR error: 0.2700 (train), 0.1972 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 12                                                      \n","LOSSES: 0.9771 (train), 0.9767 (val)\n","OD segmentation (Dice Score): 0.5777 (train), 0.6921 (val)\n","OC segmentation (Dice Score): 0.4214 (train), 0.5517 (val)\n","vCDR error: 0.2618 (train), 0.1821 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 13                                                      \n","LOSSES: 0.9771 (train), 0.9766 (val)\n","OD segmentation (Dice Score): 0.5810 (train), 0.6385 (val)\n","OC segmentation (Dice Score): 0.4209 (train), 0.5041 (val)\n","vCDR error: 0.2762 (train), 0.1755 (val)\n","__________________________________________________\n","VALIDATION epoch 14                                                      \n","LOSSES: 0.9770 (train), 0.9766 (val)\n","OD segmentation (Dice Score): 0.6003 (train), 0.5867 (val)\n","OC segmentation (Dice Score): 0.4320 (train), 0.4612 (val)\n","vCDR error: 0.2658 (train), 0.1797 (val)\n","__________________________________________________\n","VALIDATION epoch 15                                                      \n","LOSSES: 0.9771 (train), 0.9766 (val)\n","OD segmentation (Dice Score): 0.5732 (train), 0.5499 (val)\n","OC segmentation (Dice Score): 0.4251 (train), 0.4154 (val)\n","vCDR error: 0.2820 (train), 0.1854 (val)\n","__________________________________________________\n","VALIDATION epoch 16                                                      \n","LOSSES: 0.9771 (train), 0.9766 (val)\n","OD segmentation (Dice Score): 0.5841 (train), 0.6738 (val)\n","OC segmentation (Dice Score): 0.4281 (train), 0.5064 (val)\n","vCDR error: 0.2723 (train), 0.1941 (val)\n","__________________________________________________\n","VALIDATION epoch 17                                                      \n","LOSSES: 0.9771 (train), 0.9772 (val)\n","OD segmentation (Dice Score): 0.5704 (train), 0.4722 (val)\n","OC segmentation (Dice Score): 0.4101 (train), 0.4145 (val)\n","vCDR error: 0.2820 (train), 0.1728 (val)\n","__________________________________________________\n","VALIDATION epoch 18                                                      \n","LOSSES: 0.9771 (train), 0.9767 (val)\n","OD segmentation (Dice Score): 0.5544 (train), 0.5159 (val)\n","OC segmentation (Dice Score): 0.4055 (train), 0.3790 (val)\n","vCDR error: 0.2770 (train), 0.2469 (val)\n","__________________________________________________\n","VALIDATION epoch 19                                                      \n","LOSSES: 0.9771 (train), 0.9767 (val)\n","OD segmentation (Dice Score): 0.5446 (train), 0.6845 (val)\n","OC segmentation (Dice Score): 0.4093 (train), 0.5919 (val)\n","vCDR error: 0.2839 (train), 0.1406 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 20                                                      \n","LOSSES: 0.9771 (train), 0.9767 (val)\n","OD segmentation (Dice Score): 0.5874 (train), 0.4512 (val)\n","OC segmentation (Dice Score): 0.4250 (train), 0.3564 (val)\n","vCDR error: 0.2661 (train), 0.1912 (val)\n","__________________________________________________\n","Train time (total): 2730.74542427063\n","Validation time (total): 663.0189673900604\n","Validation time (avg): 0.027625790307919184/img\n"]}],"source":["# Define parameters\n","nb_train_batches = len(train_loader)\n","nb_val_batches = len(val_loader)\n","nb_iter = 0\n","best_val_auc = 0.\n","iters = list(range(1, 10))\n","val_losses = []\n","train_losses = []\n","train_accuracy=[]\n","val_accuracy=[]\n","list_train_time = []\n","list_val_time = []\n","epoch = 0\n","\n","model = UNet(n_channels=3, n_classes=2).to(device)\n","# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","while epoch < total_epoch:\n","    # Accumulators\n","    train_vCDRs, val_vCDRs = [], []\n","    train_loss, val_loss = 0., 0.\n","    train_dsc_od, val_dsc_od = 0., 0.\n","    train_dsc_oc, val_dsc_oc = 0., 0.\n","    train_vCDR_error, val_vCDR_error = 0., 0.\n","    \n","    ############\n","    # TRAINING #\n","    ############\n","    start_train = time.time()\n","    model.train()\n","    train_data = iter(train_loader)\n","    for k in range(nb_train_batches):\n","        # Loads data\n","        imgs, seg_gts = train_data.next()\n","        imgs, seg_gts = imgs.to(device), seg_gts.to(device)\n","\n","        # Forward pass\n","        logits = model(imgs)\n","        loss = seg_loss(logits, seg_gts)\n"," \n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() / nb_train_batches\n","        #for printing the loss curves\n","        \n","        train_losses.append(train_loss)\n","        \n","        with torch.no_grad():\n","            # Compute segmentation metric\n","            pred_od = refine_seg((logits[:,0,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            #pred_od = refine_seg((logits[:,0,:,:]>=0.5).type(torch.int8).cpu())\n","            pred_oc = refine_seg((logits[:,1,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            #pred_oc = refine_seg((logits[:,1,:,:]>=0.5).type(torch.int8).cpu())\n","            gt_od = seg_gts[:,0,:,:].type(torch.int8)\n","            gt_oc = seg_gts[:,1,:,:].type(torch.int8)\n","            dsc_od = compute_dice_coef(pred_od, gt_od)\n","            dsc_oc = compute_dice_coef(pred_oc, gt_oc)\n","            train_dsc_od += dsc_od.item()/nb_train_batches\n","            train_dsc_oc += dsc_oc.item()/nb_train_batches\n","\n","\n","            # Compute and store vCDRs\n","            vCDR_error, pred_vCDR, gt_vCDR = compute_vCDR_error(pred_od.cpu().numpy(), pred_oc.cpu().numpy(), gt_od.cpu().numpy(), gt_oc.cpu().numpy())\n","            train_vCDRs += pred_vCDR.tolist()\n","            train_vCDR_error += vCDR_error  / nb_train_batches\n","            \n","        # Increase iterations\n","        nb_iter += 1\n","        \n","        # Std out\n","        print('Epoch {}, iter {}/{}, loss {:.6f}'.format(epoch+1, k+1, nb_train_batches, loss.item()) + ' '*20, \n","              end='\\r')\n","    end_train = time.time()\n","    \n","    \n","    ##############\n","    # VALIDATION #\n","    ##############\n","    start_val = time.time()\n","    model.eval()\n","    with torch.no_grad():\n","        val_data = iter(val_loader)\n","        for k in range(nb_val_batches):\n","            # Loads data\n","            imgs, seg_gts = val_data.next()\n","            imgs, seg_gts = imgs.to(device), seg_gts.to(device)\n","            \n","            # Forward pass\n","            logits = model(imgs)\n","            val_loss += seg_loss(logits, seg_gts).item() / nb_val_batches\n","            \n","            val_losses.append(val_loss)\n","\n","            # Std out\n","            print('Validation iter {}/{}'.format(k+1, nb_val_batches) + ' '*50, \n","                  end='\\r')\n","            \n","            # Compute segmentation metric\n","            pred_od = refine_seg((logits[:,0,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            pred_oc = refine_seg((logits[:,1,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            gt_od = seg_gts[:,0,:,:].type(torch.int8)\n","            gt_oc = seg_gts[:,1,:,:].type(torch.int8)\n","            dsc_od = compute_dice_coef(pred_od, gt_od)\n","            dsc_oc = compute_dice_coef(pred_oc, gt_oc)\n","            val_dsc_od += dsc_od.item()/nb_val_batches\n","            val_dsc_oc += dsc_oc.item()/nb_val_batches\n","            \n","        \n","            vCDR_error, pred_vCDR, gt_vCDR = compute_vCDR_error(pred_od.cpu().numpy(), pred_oc.cpu().numpy(), gt_od.cpu().numpy(), gt_oc.cpu().numpy())\n","            val_vCDRs += pred_vCDR.tolist()\n","            val_vCDR_error += vCDR_error / nb_val_batches\n","    print('VALIDATION epoch {}'.format(epoch+1)+' '*50)\n","    print('LOSSES: {:.4f} (train), {:.4f} (val)'.format(train_loss, val_loss))\n","    print('OD segmentation (Dice Score): {:.4f} (train), {:.4f} (val)'.format(train_dsc_od, val_dsc_od))\n","    print('OC segmentation (Dice Score): {:.4f} (train), {:.4f} (val)'.format(train_dsc_oc, val_dsc_oc))\n","    print('vCDR error: {:.4f} (train), {:.4f} (val)'.format(train_vCDR_error, val_vCDR_error))\n","    # Save model if best validation AUC is reached\n","    if val_dsc_od + val_dsc_oc > best_val_auc:\n","        torch.save(model.state_dict(), '/kaggle/working/best_seg.pth')\n","        best_val_auc = val_dsc_od + val_dsc_oc\n","        best_val_dsc_od = val_dsc_od\n","        best_val_dsc_oc = val_dsc_oc\n","        print('Best validation AUC reached. Saved model weights.')\n","    print('_'*50)\n","    end_val = time.time()    \n","    \n","    list_train_time.append(end_train-start_train)\n","    list_val_time.append(end_val-start_val)\n","    # End of epoch\n","    epoch += 1\n","\n","train_time = sum(list_train_time)\n","valid_time = sum(list_val_time)/total_epoch/len(val_set)\n","dict_time = {'train_time': list_train_time, 'valid_time': list_val_time}         \n","df_time = pd.DataFrame(dict_time) \n","df_time.to_csv('UNet_time.csv', index=False) \n","\n","print(f'Train time (total): {train_time}')\n","print(f'Validation time (total): {sum(list_val_time)}')\n","print(f'Validation time (avg): {valid_time}/img')\n","total_params = get_n_params(model)\n","save_report('UNet', best_val_dsc_od, best_val_dsc_oc, best_val_auc, train_time, valid_time, total_params)\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T13:24:11.629156Z","iopub.status.busy":"2023-01-14T13:24:11.628533Z","iopub.status.idle":"2023-01-14T14:18:06.648839Z","shell.execute_reply":"2023-01-14T14:18:06.647319Z","shell.execute_reply.started":"2023-01-14T13:24:11.629116Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["VALIDATION epoch 1                                                       \n","LOSSES: 0.9334 (train), 0.8088 (val)\n","OD segmentation (Dice Score): 0.3818 (train), 0.4312 (val)\n","OC segmentation (Dice Score): 0.2097 (train), 0.5331 (val)\n","vCDR error: 0.4854 (train), 1.0750 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 2                                                       \n","LOSSES: 0.3970 (train), 0.2794 (val)\n","OD segmentation (Dice Score): 0.6321 (train), 0.7044 (val)\n","OC segmentation (Dice Score): 0.5284 (train), 0.7634 (val)\n","vCDR error: 1.0888 (train), 0.1711 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 3                                                       \n","LOSSES: 0.2822 (train), 0.3563 (val)\n","OD segmentation (Dice Score): 0.7137 (train), 0.7029 (val)\n","OC segmentation (Dice Score): 0.6160 (train), 0.2729 (val)\n","vCDR error: 0.8126 (train), 1.8452 (val)\n","__________________________________________________\n","VALIDATION epoch 4                                                       \n","LOSSES: 0.2281 (train), 0.2451 (val)\n","OD segmentation (Dice Score): 0.7788 (train), 0.7634 (val)\n","OC segmentation (Dice Score): 0.6281 (train), 0.7138 (val)\n","vCDR error: 0.8306 (train), 0.1896 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 5                                                       \n","LOSSES: 0.2037 (train), 0.2290 (val)\n","OD segmentation (Dice Score): 0.8012 (train), 0.7654 (val)\n","OC segmentation (Dice Score): 0.6481 (train), 0.7677 (val)\n","vCDR error: 0.6320 (train), 0.1171 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 6                                                       \n","LOSSES: 0.1809 (train), 0.1985 (val)\n","OD segmentation (Dice Score): 0.8233 (train), 0.7871 (val)\n","OC segmentation (Dice Score): 0.6664 (train), 0.8162 (val)\n","vCDR error: 0.7700 (train), 0.1310 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 7                                                       \n","LOSSES: 0.1717 (train), 0.2290 (val)\n","OD segmentation (Dice Score): 0.8321 (train), 0.7656 (val)\n","OC segmentation (Dice Score): 0.6729 (train), 0.7502 (val)\n","vCDR error: 0.6598 (train), 0.2659 (val)\n","__________________________________________________\n","VALIDATION epoch 8                                                       \n","LOSSES: 0.1586 (train), 0.2003 (val)\n","OD segmentation (Dice Score): 0.8457 (train), 0.7938 (val)\n","OC segmentation (Dice Score): 0.6864 (train), 0.7910 (val)\n","vCDR error: 0.6092 (train), 0.2220 (val)\n","__________________________________________________\n","VALIDATION epoch 9                                                       \n","LOSSES: 0.1519 (train), 0.1925 (val)\n","OD segmentation (Dice Score): 0.8515 (train), 0.7986 (val)\n","OC segmentation (Dice Score): 0.6932 (train), 0.7963 (val)\n","vCDR error: 0.5994 (train), 0.2289 (val)\n","__________________________________________________\n","VALIDATION epoch 10                                                      \n","LOSSES: 0.1478 (train), 0.1775 (val)\n","OD segmentation (Dice Score): 0.8555 (train), 0.8119 (val)\n","OC segmentation (Dice Score): 0.7019 (train), 0.8211 (val)\n","vCDR error: 0.5957 (train), 0.2132 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 11                                                      \n","LOSSES: 0.1473 (train), 0.1841 (val)\n","OD segmentation (Dice Score): 0.8565 (train), 0.7977 (val)\n","OC segmentation (Dice Score): 0.6957 (train), 0.8062 (val)\n","vCDR error: 0.8090 (train), 0.4899 (val)\n","__________________________________________________\n","VALIDATION epoch 12                                                      \n","LOSSES: 0.1374 (train), 0.1717 (val)\n","OD segmentation (Dice Score): 0.8652 (train), 0.8199 (val)\n","OC segmentation (Dice Score): 0.7038 (train), 0.8220 (val)\n","vCDR error: 0.7161 (train), 0.1319 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 13                                                      \n","LOSSES: 0.1298 (train), 0.1756 (val)\n","OD segmentation (Dice Score): 0.8708 (train), 0.8267 (val)\n","OC segmentation (Dice Score): 0.7119 (train), 0.8085 (val)\n","vCDR error: 0.7939 (train), 0.1371 (val)\n","__________________________________________________\n","VALIDATION epoch 14                                                      \n","LOSSES: 0.1308 (train), 0.1745 (val)\n","OD segmentation (Dice Score): 0.8694 (train), 0.8291 (val)\n","OC segmentation (Dice Score): 0.7123 (train), 0.8063 (val)\n","vCDR error: 0.8508 (train), 0.1925 (val)\n","__________________________________________________\n","VALIDATION epoch 15                                                      \n","LOSSES: 0.1231 (train), 0.1641 (val)\n","OD segmentation (Dice Score): 0.8785 (train), 0.8265 (val)\n","OC segmentation (Dice Score): 0.7204 (train), 0.8377 (val)\n","vCDR error: 0.8038 (train), 0.1904 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 16                                                      \n","LOSSES: 0.1155 (train), 0.1880 (val)\n","OD segmentation (Dice Score): 0.8855 (train), 0.7969 (val)\n","OC segmentation (Dice Score): 0.7300 (train), 0.8145 (val)\n","vCDR error: 0.8832 (train), 0.4237 (val)\n","__________________________________________________\n","VALIDATION epoch 17                                                      \n","LOSSES: 0.1098 (train), 0.1826 (val)\n","OD segmentation (Dice Score): 0.8899 (train), 0.7983 (val)\n","OC segmentation (Dice Score): 0.7383 (train), 0.8363 (val)\n","vCDR error: 0.8549 (train), 0.1574 (val)\n","__________________________________________________\n","VALIDATION epoch 18                                                      \n","LOSSES: 0.1058 (train), 0.1838 (val)\n","OD segmentation (Dice Score): 0.8923 (train), 0.8084 (val)\n","OC segmentation (Dice Score): 0.7438 (train), 0.8139 (val)\n","vCDR error: 1.0693 (train), 0.1435 (val)\n","__________________________________________________\n","VALIDATION epoch 19                                                      \n","LOSSES: 0.1043 (train), 0.1578 (val)\n","OD segmentation (Dice Score): 0.8942 (train), 0.8488 (val)\n","OC segmentation (Dice Score): 0.7426 (train), 0.8133 (val)\n","vCDR error: 1.0032 (train), 0.1911 (val)\n","__________________________________________________\n","VALIDATION epoch 20                                                      \n","LOSSES: 0.1005 (train), 0.1949 (val)\n","OD segmentation (Dice Score): 0.8978 (train), 0.7835 (val)\n","OC segmentation (Dice Score): 0.7473 (train), 0.8332 (val)\n","vCDR error: 0.9819 (train), 0.1754 (val)\n","__________________________________________________\n","Train time (total): 2591.616738319397\n","Validation time (total): 642.8466882705688\n","Validation time (avg): 0.02678527867794037/img\n"]}],"source":["# Define parameters\n","nb_train_batches = len(train_loader)\n","nb_val_batches = len(val_loader)\n","nb_iter = 0\n","best_val_auc = 0.\n","iters = list(range(1, 10))\n","val_losses = []\n","train_losses = []\n","train_accuracy=[]\n","val_accuracy=[]\n","list_train_time = []\n","list_val_time = []\n","epoch = 0\n","\n","model = AttU_Net(n_channels=3, n_classes=2).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n","while epoch < total_epoch:\n","    # Accumulators\n","    train_vCDRs, val_vCDRs = [], []\n","    train_loss, val_loss = 0., 0.\n","    train_dsc_od, val_dsc_od = 0., 0.\n","    train_dsc_oc, val_dsc_oc = 0., 0.\n","    train_vCDR_error, val_vCDR_error = 0., 0.\n","    \n","    ############\n","    # TRAINING #\n","    ############\n","    start_train = time.time()\n","    model.train()\n","    train_data = iter(train_loader)\n","    for k in range(nb_train_batches):\n","        # Loads data\n","        imgs, seg_gts = train_data.next()\n","        imgs, seg_gts = imgs.to(device), seg_gts.to(device)\n","\n","        # Forward pass\n","        logits = model(imgs)\n","        loss = seg_loss(logits, seg_gts)\n"," \n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() / nb_train_batches\n","        #for printing the loss curves\n","        \n","        train_losses.append(train_loss)\n","        \n","        with torch.no_grad():\n","            # Compute segmentation metric\n","            pred_od = refine_seg((logits[:,0,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            #pred_od = refine_seg((logits[:,0,:,:]>=0.5).type(torch.int8).cpu())\n","            pred_oc = refine_seg((logits[:,1,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            #pred_oc = refine_seg((logits[:,1,:,:]>=0.5).type(torch.int8).cpu())\n","            gt_od = seg_gts[:,0,:,:].type(torch.int8)\n","            gt_oc = seg_gts[:,1,:,:].type(torch.int8)\n","            dsc_od = compute_dice_coef(pred_od, gt_od)\n","            dsc_oc = compute_dice_coef(pred_oc, gt_oc)\n","            train_dsc_od += dsc_od.item()/nb_train_batches\n","            train_dsc_oc += dsc_oc.item()/nb_train_batches\n","\n","\n","            # Compute and store vCDRs\n","            vCDR_error, pred_vCDR, gt_vCDR = compute_vCDR_error(pred_od.cpu().numpy(), pred_oc.cpu().numpy(), gt_od.cpu().numpy(), gt_oc.cpu().numpy())\n","            train_vCDRs += pred_vCDR.tolist()\n","            train_vCDR_error += vCDR_error  / nb_train_batches\n","            \n","        # Increase iterations\n","        nb_iter += 1\n","        \n","        # Std out\n","        print('Epoch {}, iter {}/{}, loss {:.6f}'.format(epoch+1, k+1, nb_train_batches, loss.item()) + ' '*20, \n","              end='\\r')\n","    end_train = time.time()\n","    \n","    \n","    ##############\n","    # VALIDATION #\n","    ##############\n","    start_val = time.time()\n","    model.eval()\n","    with torch.no_grad():\n","        val_data = iter(val_loader)\n","        for k in range(nb_val_batches):\n","            # Loads data\n","            imgs, seg_gts = val_data.next()\n","            imgs, seg_gts = imgs.to(device), seg_gts.to(device)\n","            \n","            # Forward pass\n","            logits = model(imgs)\n","            val_loss += seg_loss(logits, seg_gts).item() / nb_val_batches\n","            \n","            val_losses.append(val_loss)\n","\n","            # Std out\n","            print('Validation iter {}/{}'.format(k+1, nb_val_batches) + ' '*50, \n","                  end='\\r')\n","            \n","            # Compute segmentation metric\n","            pred_od = refine_seg((logits[:,0,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            pred_oc = refine_seg((logits[:,1,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            gt_od = seg_gts[:,0,:,:].type(torch.int8)\n","            gt_oc = seg_gts[:,1,:,:].type(torch.int8)\n","            dsc_od = compute_dice_coef(pred_od, gt_od)\n","            dsc_oc = compute_dice_coef(pred_oc, gt_oc)\n","            val_dsc_od += dsc_od.item()/nb_val_batches\n","            val_dsc_oc += dsc_oc.item()/nb_val_batches\n","            \n","        \n","            vCDR_error, pred_vCDR, gt_vCDR = compute_vCDR_error(pred_od.cpu().numpy(), pred_oc.cpu().numpy(), gt_od.cpu().numpy(), gt_oc.cpu().numpy())\n","            val_vCDRs += pred_vCDR.tolist()\n","            val_vCDR_error += vCDR_error / nb_val_batches\n","    print('VALIDATION epoch {}'.format(epoch+1)+' '*50)\n","    print('LOSSES: {:.4f} (train), {:.4f} (val)'.format(train_loss, val_loss))\n","    print('OD segmentation (Dice Score): {:.4f} (train), {:.4f} (val)'.format(train_dsc_od, val_dsc_od))\n","    print('OC segmentation (Dice Score): {:.4f} (train), {:.4f} (val)'.format(train_dsc_oc, val_dsc_oc))\n","    print('vCDR error: {:.4f} (train), {:.4f} (val)'.format(train_vCDR_error, val_vCDR_error))\n","    # Save model if best validation AUC is reached\n","    if val_dsc_od + val_dsc_oc > best_val_auc:\n","        torch.save(model.state_dict(), '/kaggle/working/best_seg.pth')\n","        best_val_auc = val_dsc_od + val_dsc_oc\n","        best_val_dsc_od = val_dsc_od\n","        best_val_dsc_oc = val_dsc_oc\n","        print('Best validation AUC reached. Saved model weights.')\n","    print('_'*50)\n","    end_val = time.time()    \n","    \n","    list_train_time.append(end_train-start_train)\n","    list_val_time.append(end_val-start_val)\n","    # End of epoch\n","    epoch += 1\n","\n","train_time = sum(list_train_time)\n","valid_time = sum(list_val_time)/total_epoch/len(val_set)\n","dict_time = {'train_time': list_train_time, 'valid_time': list_val_time}         \n","df_time = pd.DataFrame(dict_time) \n","df_time.to_csv('Attention_UNet_time.csv', index=False) \n","\n","print(f'Train time (total): {train_time}')\n","print(f'Validation time (total): {sum(list_val_time)}')\n","print(f'Validation time (avg): {valid_time}/img')\n","total_params = get_n_params(model)\n","save_report('Attention_UNet', best_val_dsc_od, best_val_dsc_oc, best_val_auc, train_time, valid_time, total_params)\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T14:18:06.652259Z","iopub.status.busy":"2023-01-14T14:18:06.651819Z","iopub.status.idle":"2023-01-14T14:56:20.993973Z","shell.execute_reply":"2023-01-14T14:56:20.992435Z","shell.execute_reply.started":"2023-01-14T14:18:06.652220Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["VALIDATION epoch 1                                                       \n","LOSSES: 0.9012 (train), 0.6985 (val)\n","OD segmentation (Dice Score): 0.3271 (train), 0.4667 (val)\n","OC segmentation (Dice Score): 0.2150 (train), 0.2241 (val)\n","vCDR error: 0.6840 (train), 0.3770 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 2                                                       \n","LOSSES: 0.3196 (train), 0.2884 (val)\n","OD segmentation (Dice Score): 0.6754 (train), 0.6959 (val)\n","OC segmentation (Dice Score): 0.5889 (train), 0.7771 (val)\n","vCDR error: 0.8222 (train), 0.1835 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 3                                                       \n","LOSSES: 0.2607 (train), 0.1951 (val)\n","OD segmentation (Dice Score): 0.7347 (train), 0.8053 (val)\n","OC segmentation (Dice Score): 0.6263 (train), 0.7984 (val)\n","vCDR error: 0.7055 (train), 0.1129 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 4                                                       \n","LOSSES: 0.2027 (train), 0.1834 (val)\n","OD segmentation (Dice Score): 0.8013 (train), 0.8118 (val)\n","OC segmentation (Dice Score): 0.6513 (train), 0.8126 (val)\n","vCDR error: 0.7844 (train), 0.1268 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 5                                                       \n","LOSSES: 0.1883 (train), 0.1746 (val)\n","OD segmentation (Dice Score): 0.8145 (train), 0.8302 (val)\n","OC segmentation (Dice Score): 0.6644 (train), 0.7943 (val)\n","vCDR error: 0.7275 (train), 0.1058 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 6                                                       \n","LOSSES: 0.1702 (train), 0.1539 (val)\n","OD segmentation (Dice Score): 0.8310 (train), 0.8426 (val)\n","OC segmentation (Dice Score): 0.6793 (train), 0.8391 (val)\n","vCDR error: 0.6809 (train), 0.0828 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 7                                                       \n","LOSSES: 0.1654 (train), 0.2203 (val)\n","OD segmentation (Dice Score): 0.8371 (train), 0.8019 (val)\n","OC segmentation (Dice Score): 0.6863 (train), 0.6578 (val)\n","vCDR error: 0.7075 (train), 0.3184 (val)\n","__________________________________________________\n","VALIDATION epoch 8                                                       \n","LOSSES: 0.1585 (train), 0.1671 (val)\n","OD segmentation (Dice Score): 0.8454 (train), 0.8275 (val)\n","OC segmentation (Dice Score): 0.6884 (train), 0.8228 (val)\n","vCDR error: 0.6566 (train), 0.1172 (val)\n","__________________________________________________\n","VALIDATION epoch 9                                                       \n","LOSSES: 0.1515 (train), 0.1509 (val)\n","OD segmentation (Dice Score): 0.8511 (train), 0.8500 (val)\n","OC segmentation (Dice Score): 0.6938 (train), 0.8318 (val)\n","vCDR error: 0.6860 (train), 0.1035 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 10                                                      \n","LOSSES: 0.1428 (train), 0.1620 (val)\n","OD segmentation (Dice Score): 0.8585 (train), 0.8352 (val)\n","OC segmentation (Dice Score): 0.7065 (train), 0.8285 (val)\n","vCDR error: 0.7405 (train), 0.0954 (val)\n","__________________________________________________\n","VALIDATION epoch 11                                                      \n","LOSSES: 0.1373 (train), 0.1585 (val)\n","OD segmentation (Dice Score): 0.8645 (train), 0.8275 (val)\n","OC segmentation (Dice Score): 0.7093 (train), 0.8447 (val)\n","vCDR error: 0.8330 (train), 0.1005 (val)\n","__________________________________________________\n","VALIDATION epoch 12                                                      \n","LOSSES: 0.1340 (train), 0.1640 (val)\n","OD segmentation (Dice Score): 0.8665 (train), 0.8208 (val)\n","OC segmentation (Dice Score): 0.7118 (train), 0.8461 (val)\n","vCDR error: 0.8275 (train), 0.1428 (val)\n","__________________________________________________\n","VALIDATION epoch 13                                                      \n","LOSSES: 0.1277 (train), 0.1748 (val)\n","OD segmentation (Dice Score): 0.8731 (train), 0.8095 (val)\n","OC segmentation (Dice Score): 0.7196 (train), 0.8205 (val)\n","vCDR error: 0.8045 (train), 0.2374 (val)\n","__________________________________________________\n","VALIDATION epoch 14                                                      \n","LOSSES: 0.1352 (train), 0.1785 (val)\n","OD segmentation (Dice Score): 0.8666 (train), 0.8074 (val)\n","OC segmentation (Dice Score): 0.7110 (train), 0.8402 (val)\n","vCDR error: 0.8083 (train), 0.1029 (val)\n","__________________________________________________\n","VALIDATION epoch 15                                                      \n","LOSSES: 0.1265 (train), 0.1560 (val)\n","OD segmentation (Dice Score): 0.8741 (train), 0.8353 (val)\n","OC segmentation (Dice Score): 0.7192 (train), 0.8412 (val)\n","vCDR error: 0.8100 (train), 0.1047 (val)\n","__________________________________________________\n","VALIDATION epoch 16                                                      \n","LOSSES: 0.1191 (train), 0.1705 (val)\n","OD segmentation (Dice Score): 0.8811 (train), 0.8164 (val)\n","OC segmentation (Dice Score): 0.7286 (train), 0.8320 (val)\n","vCDR error: 0.8942 (train), 0.1270 (val)\n","__________________________________________________\n","VALIDATION epoch 17                                                      \n","LOSSES: 0.1168 (train), 0.1635 (val)\n","OD segmentation (Dice Score): 0.8830 (train), 0.8297 (val)\n","OC segmentation (Dice Score): 0.7277 (train), 0.8319 (val)\n","vCDR error: 0.8626 (train), 0.0995 (val)\n","__________________________________________________\n","VALIDATION epoch 18                                                      \n","LOSSES: 0.1317 (train), 0.1508 (val)\n","OD segmentation (Dice Score): 0.8677 (train), 0.8420 (val)\n","OC segmentation (Dice Score): 0.7189 (train), 0.8478 (val)\n","vCDR error: 0.9519 (train), 0.0946 (val)\n","Best validation AUC reached. Saved model weights.\n","__________________________________________________\n","VALIDATION epoch 19                                                      \n","LOSSES: 0.1183 (train), 0.1567 (val)\n","OD segmentation (Dice Score): 0.8810 (train), 0.8337 (val)\n","OC segmentation (Dice Score): 0.7264 (train), 0.8471 (val)\n","vCDR error: 0.9093 (train), 0.0958 (val)\n","__________________________________________________\n","VALIDATION epoch 20                                                      \n","LOSSES: 0.1133 (train), 0.1670 (val)\n","OD segmentation (Dice Score): 0.8856 (train), 0.8170 (val)\n","OC segmentation (Dice Score): 0.7323 (train), 0.8408 (val)\n","vCDR error: 0.8962 (train), 0.1307 (val)\n","__________________________________________________\n","Train time (total): 1804.0300951004028\n","Validation time (total): 488.51052474975586\n","Validation time (avg): 0.020354605197906495/img\n"]}],"source":["# Define parameters\n","nb_train_batches = len(train_loader)\n","nb_val_batches = len(val_loader)\n","nb_iter = 0\n","best_val_auc = 0.\n","iters = list(range(1, 10))\n","val_losses = []\n","train_losses = []\n","train_accuracy=[]\n","val_accuracy=[]\n","list_train_time = []\n","list_val_time = []\n","epoch = 0\n","\n","model = VisionTransformer(config_vit, img_size=zzimg_size, num_classes=config_vit.n_classes).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n","while epoch < total_epoch:\n","    # Accumulators\n","    train_vCDRs, val_vCDRs = [], []\n","    train_loss, val_loss = 0., 0.\n","    train_dsc_od, val_dsc_od = 0., 0.\n","    train_dsc_oc, val_dsc_oc = 0., 0.\n","    train_vCDR_error, val_vCDR_error = 0., 0.\n","    \n","    ############\n","    # TRAINING #\n","    ############\n","    start_train = time.time()\n","    model.train()\n","    train_data = iter(train_loader)\n","    for k in range(nb_train_batches):\n","        # Loads data\n","        imgs, seg_gts = train_data.next()\n","        imgs, seg_gts = imgs.to(device), seg_gts.to(device)\n","\n","        # Forward pass\n","        logits = model(imgs)\n","        loss = seg_loss(logits, seg_gts)\n"," \n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() / nb_train_batches\n","        #for printing the loss curves\n","        \n","        train_losses.append(train_loss)\n","        \n","        with torch.no_grad():\n","            # Compute segmentation metric\n","            pred_od = refine_seg((logits[:,0,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            #pred_od = refine_seg((logits[:,0,:,:]>=0.5).type(torch.int8).cpu())\n","            pred_oc = refine_seg((logits[:,1,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            #pred_oc = refine_seg((logits[:,1,:,:]>=0.5).type(torch.int8).cpu())\n","            gt_od = seg_gts[:,0,:,:].type(torch.int8)\n","            gt_oc = seg_gts[:,1,:,:].type(torch.int8)\n","            dsc_od = compute_dice_coef(pred_od, gt_od)\n","            dsc_oc = compute_dice_coef(pred_oc, gt_oc)\n","            train_dsc_od += dsc_od.item()/nb_train_batches\n","            train_dsc_oc += dsc_oc.item()/nb_train_batches\n","\n","\n","            # Compute and store vCDRs\n","            vCDR_error, pred_vCDR, gt_vCDR = compute_vCDR_error(pred_od.cpu().numpy(), pred_oc.cpu().numpy(), gt_od.cpu().numpy(), gt_oc.cpu().numpy())\n","            train_vCDRs += pred_vCDR.tolist()\n","            train_vCDR_error += vCDR_error  / nb_train_batches\n","            \n","        # Increase iterations\n","        nb_iter += 1\n","        \n","        # Std out\n","        print('Epoch {}, iter {}/{}, loss {:.6f}'.format(epoch+1, k+1, nb_train_batches, loss.item()) + ' '*20, \n","              end='\\r')\n","    end_train = time.time()\n","    \n","    \n","    ##############\n","    # VALIDATION #\n","    ##############\n","    start_val = time.time()\n","    model.eval()\n","    with torch.no_grad():\n","        val_data = iter(val_loader)\n","        for k in range(nb_val_batches):\n","            # Loads data\n","            imgs, seg_gts = val_data.next()\n","            imgs, seg_gts = imgs.to(device), seg_gts.to(device)\n","            \n","            # Forward pass\n","            logits = model(imgs)\n","            val_loss += seg_loss(logits, seg_gts).item() / nb_val_batches\n","            \n","            val_losses.append(val_loss)\n","\n","            # Std out\n","            print('Validation iter {}/{}'.format(k+1, nb_val_batches) + ' '*50, \n","                  end='\\r')\n","            \n","            # Compute segmentation metric\n","            pred_od = refine_seg((logits[:,0,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            pred_oc = refine_seg((logits[:,1,:,:]>=0.5).type(torch.int8).cpu()).to(device)\n","            gt_od = seg_gts[:,0,:,:].type(torch.int8)\n","            gt_oc = seg_gts[:,1,:,:].type(torch.int8)\n","            dsc_od = compute_dice_coef(pred_od, gt_od)\n","            dsc_oc = compute_dice_coef(pred_oc, gt_oc)\n","            val_dsc_od += dsc_od.item()/nb_val_batches\n","            val_dsc_oc += dsc_oc.item()/nb_val_batches\n","            \n","        \n","            vCDR_error, pred_vCDR, gt_vCDR = compute_vCDR_error(pred_od.cpu().numpy(), pred_oc.cpu().numpy(), gt_od.cpu().numpy(), gt_oc.cpu().numpy())\n","            val_vCDRs += pred_vCDR.tolist()\n","            val_vCDR_error += vCDR_error / nb_val_batches\n","    print('VALIDATION epoch {}'.format(epoch+1)+' '*50)\n","    print('LOSSES: {:.4f} (train), {:.4f} (val)'.format(train_loss, val_loss))\n","    print('OD segmentation (Dice Score): {:.4f} (train), {:.4f} (val)'.format(train_dsc_od, val_dsc_od))\n","    print('OC segmentation (Dice Score): {:.4f} (train), {:.4f} (val)'.format(train_dsc_oc, val_dsc_oc))\n","    print('vCDR error: {:.4f} (train), {:.4f} (val)'.format(train_vCDR_error, val_vCDR_error))\n","    # Save model if best validation AUC is reached\n","    if val_dsc_od + val_dsc_oc > best_val_auc:\n","        torch.save(model.state_dict(), '/kaggle/working/best_seg.pth')\n","        best_val_auc = val_dsc_od + val_dsc_oc\n","        best_val_dsc_od = val_dsc_od\n","        best_val_dsc_oc = val_dsc_oc\n","        print('Best validation AUC reached. Saved model weights.')\n","    print('_'*50)\n","    end_val = time.time()    \n","    \n","    list_train_time.append(end_train-start_train)\n","    list_val_time.append(end_val-start_val)\n","    # End of epoch\n","    epoch += 1\n","\n","train_time = sum(list_train_time)\n","valid_time = sum(list_val_time)/total_epoch/len(val_set)\n","dict_time = {'train_time': list_train_time, 'valid_time': list_val_time}         \n","df_time = pd.DataFrame(dict_time) \n","df_time.to_csv('TransUNet_time.csv', index=False) \n","\n","print(f'Train time (total): {train_time}')\n","print(f'Validation time (total): {sum(list_val_time)}')\n","print(f'Validation time (avg): {valid_time}/img')\n","total_params = get_n_params(model)\n","save_report('TransUNet', best_val_dsc_od, best_val_dsc_oc, best_val_auc, train_time, valid_time, total_params)\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-01-14T14:56:20.997569Z","iopub.status.busy":"2023-01-14T14:56:20.996671Z","iopub.status.idle":"2023-01-14T14:56:21.027319Z","shell.execute_reply":"2023-01-14T14:56:21.026249Z","shell.execute_reply.started":"2023-01-14T14:56:20.997527Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>best_od_dice</th>\n","      <th>best_oc_dice</th>\n","      <th>avg_dice</th>\n","      <th>best_auc</th>\n","      <th>train_time</th>\n","      <th>valid_time</th>\n","      <th>total_params</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>UNet</td>\n","      <td>0.684520</td>\n","      <td>0.591909</td>\n","      <td>0.638215</td>\n","      <td>1.276430</td>\n","      <td>2730.745424</td>\n","      <td>0.027626</td>\n","      <td>276831490</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Attention_UNet</td>\n","      <td>0.826460</td>\n","      <td>0.837713</td>\n","      <td>0.832087</td>\n","      <td>1.664173</td>\n","      <td>2591.616738</td>\n","      <td>0.026785</td>\n","      <td>34878638</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TransUNet</td>\n","      <td>0.842034</td>\n","      <td>0.847792</td>\n","      <td>0.844913</td>\n","      <td>1.689826</td>\n","      <td>1804.030095</td>\n","      <td>0.020355</td>\n","      <td>105322146</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            model  best_od_dice  best_oc_dice  avg_dice  best_auc  \\\n","0            UNet      0.684520      0.591909  0.638215  1.276430   \n","1  Attention_UNet      0.826460      0.837713  0.832087  1.664173   \n","2       TransUNet      0.842034      0.847792  0.844913  1.689826   \n","\n","    train_time  valid_time  total_params  \n","0  2730.745424    0.027626     276831490  \n","1  2591.616738    0.026785      34878638  \n","2  1804.030095    0.020355     105322146  "]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('./best_score.csv')\n","df"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
